{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n",
    "Is a popular library for NLP that provides a wide range of features for text processing and analysis, including tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spacy we can create our own pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank model in portuguese with spacy\n",
    "nlp = spacy.blank('pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A blank model contains the basic components needed for processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sentence that we will use in this blank pipeline\n",
    "doc = nlp('Oi eu sou goku.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oi eu sou goku.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see the text with the method text\n",
    "doc.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also with the sentence in the pipeline we can use a lot of methods, and all our sentence now is tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Oi"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an example of a token in the text\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Oi\n",
      "False eu\n",
      "False sou\n",
      "False goku\n",
      "True .\n"
     ]
    }
   ],
   "source": [
    "# Also all the tokens has their own methods\n",
    "for token in doc:\n",
    "    # This method verify if the token is a punctuation\n",
    "    print(token.is_punct, token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use a trained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the a trained pipeline with \"spacy download\"\n",
    "#!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this pipeline we will identify the POS tags in our setences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All POS part-of-speech tags.\n",
    "\n",
    "* \"ADJ\": Adjective\n",
    "    * A word that describes or modifies a noun or pronoun.\n",
    "* \"ADP\": Adposition\n",
    "    * A word that shows the relationship between a noun or pronoun and other words in a sentence, such as prepositions and postpositions.\n",
    "* \"ADV\": Adverb\n",
    "    * A word that modifies a verb, adjective, or other adverb, and typically provides more information about time, place, manner, degree, etc.\n",
    "* \"AUX\": Auxiliary verb\n",
    "    * An auxiliary verb that is used with a main verb to form a verb tense, mood, or voice.\n",
    "* \"CCONJ\": Coordinating conjunction\n",
    "    * A word that connects words, phrases, or clauses of equal syntactic importance, such as \"and\", \"or\", \"but\", etc.\n",
    "* \"DET\": Determiner\n",
    "    * A word that indicates which noun is being referred to, such as articles, demonstratives, and possessives.\n",
    "* \"INTJ\": Interjection\n",
    "    * An interjection, which is a word or phrase used to express strong emotion or surprise, such as \"oh\", \"ah\", \"wow\", etc.\n",
    "* \"NOUN\": Noun\n",
    "    * \"NOUN\": A word that represents a person, place, thing, or idea.\n",
    "* \"NUM\": Numeral\n",
    "    * \"NUM\": A word that represents a number, such as \"one\", \"two\", \"three\", etc.\n",
    "* \"PART\": Particle\n",
    "    * \"PART\": A word that is used in a sentence to indicate a grammatical relationship with a neighboring word, such as \"to\" in \"going to\", or \"not\" in \"not happy\".\n",
    "* \"PRON\": Pronoun\n",
    "    * \"PRON\": A word that is used to replace a noun or noun phrase, such as \"he\", \"she\", \"it\", \"they\", etc.\n",
    "* \"PROPN\": Proper noun\n",
    "    * \"PROPN\": A proper noun, which is a specific name of a person, place, or organization, such as \"Maria\", \"Lisboa\", \"Google\", etc.\n",
    "* \"PUNCT\": Punctuation\n",
    "    * \"PUNCT\": A punctuation mark, such as \".\", \",\", \";\", etc.\n",
    "* \"SCONJ\": Subordinating conjunction\n",
    "    * A subordinating conjunction, which is a word that connects a subordinate clause to a main clause, such as \"because\", \"although\", \"if\", etc.\n",
    "* \"SYM\": Symbol\n",
    "    * A symbol, such as \"$\", \"%\", \"#\", etc.\n",
    "* \"VERB\": Verb\n",
    "    * \"VERB\": A word that expresses an action, occurrence, or state of being.\n",
    "* \"X\": Other\n",
    "    * \"X\": A catch-all category for words that do not fall into any of the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ela PRON\n",
      "gosta VERB\n",
      "de SCONJ\n",
      "comer VERB\n",
      "pizza NOUN\n",
      ", PUNCT\n",
      "na ADP\n",
      "Australia PROPN\n"
     ]
    }
   ],
   "source": [
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Ela gosta de comer pizza, na Australia\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can see the syntatic dependencies like\n",
    "\n",
    "* Subject-verb agreement: the subject noun or pronoun governs the verb in a sentence.\n",
    "* Object-verb agreement: the object noun or pronoun is governed by the verb in a sentence.\n",
    "* Adjective-noun agreement: the adjective modifies the noun in a sentence.\n",
    "* Adverb-verb agreement: the adverb modifies the verb in a sentence.\n",
    "* Preposition-object agreement: the preposition governs the object noun or pronoun in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ela nsubj\n",
      "gosta ROOT\n",
      "de mark\n",
      "comer xcomp\n",
      "pizza obj\n",
      ", punct\n",
      "na case\n",
      "Australia obl\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is a good thing to identify the labels about common words to, like name of places in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia LOC\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally sometimes is hard to identify, what each tag represent, so we can use the method explain in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non-GPE locations, mountain ranges, bodies of water'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('LOC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matcher\n",
    "We can use matcher to identify pattern in our text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'TEXT': Matches the exact text of the token, including case and whitespace.\n",
    "* 'LEMMA': Matches the base form of the token, i.e. the canonical form of the word. For example, the lemma of 'ran' is 'run'.\n",
    "* 'POS': Matches the part-of-speech tag of the token.\n",
    "* 'TAG': Matches the fine-grained part-of-speech tag of the token.\n",
    "* 'SHAPE': Matches the shape of the token, i.e. its capitalization, punctuation, and digit pattern.\n",
    "* 'ENT_TYPE': Matches the named entity type of the token.\n",
    "* 'DEP': Matches the syntactic dependency label of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[{'LOWER': 'new'}, {'LOWER': 'york'}, {'LOWER': 'city'}]]\n",
    "\n",
    "matcher.add('NYC_PATTERN', pattern)\n",
    "\n",
    "doc = nlp(\"I love New York City, but I also love San Francisco.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other example, here's a pattern that matches instances of a verb followed by a noun:\n",
    "\n",
    "```python\n",
    "pattern = [{'POS': 'VERB'}, {'POS': 'NOUN'}]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Doc manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oi Tudo Bem\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Oi\", \"Tudo\", \"Bem\"]\n",
    "spaces = [True, True, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text simlarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9607215630444086\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like frutas\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6850197911262512\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Similarity is determined using word vectors\n",
    "    * Multi-dimensional meaning representations of words\n",
    "    * Generated using an algorithm like Word2Vec and lots of text\n",
    "    * Can be added to spaCy's pipelines\n",
    "    * Default: cosine similarity, but can be adjusted\n",
    "    * Doc and Span vectors default to average of token vectors\n",
    "    * Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use Word2Vec we also can pick the generated tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14112753, -0.33611476,  0.06699206, -0.88421255,  0.06669152,\n",
       "       -0.1297453 , -0.31521478,  0.60422575,  0.4733977 , -0.64390045,\n",
       "        0.9460465 ,  1.2743245 , -0.25726676, -1.1570396 , -0.4630825 ,\n",
       "       -0.05508524, -1.2497791 , -0.51810896, -0.7528888 , -0.47276112,\n",
       "        0.2770418 ,  0.22972283, -0.00947742,  0.2889976 ,  0.28562495,\n",
       "        0.9584112 ,  0.51542306,  0.7909095 , -0.5317948 , -0.40295917,\n",
       "        0.0052537 , -0.3223989 , -0.36749154,  0.74106073,  0.384262  ,\n",
       "       -0.13837826,  0.03337788, -0.11230475,  0.15701273,  0.3177856 ,\n",
       "        0.2158667 ,  0.18146369, -0.21280894, -0.3369672 , -0.3046311 ,\n",
       "       -0.2077651 ,  0.3454957 ,  0.47986588,  0.37145495, -0.24688868,\n",
       "        0.5591996 , -0.5802103 ,  0.03678508, -0.25743204, -0.38831013,\n",
       "       -0.84508497, -0.23053315,  0.19568542, -0.15692766, -0.17506352,\n",
       "        0.56411767, -0.4119339 , -0.9519711 , -0.40832722,  0.72344995,\n",
       "       -0.68837714, -0.16142121,  0.07086641, -0.30072153, -0.2929911 ,\n",
       "       -0.24109235,  0.5909263 , -0.55706316, -0.29166397,  0.3973026 ,\n",
       "        0.01589821, -0.4454722 , -0.10357938, -0.44980475, -0.15435101,\n",
       "        0.24712597, -0.08421057, -0.21915704,  0.57924795,  0.33554238,\n",
       "        0.30591285, -0.5221469 ,  0.22823668,  0.7949787 ,  0.19089715,\n",
       "       -0.29753444, -0.09661367, -1.0756726 , -0.15070644,  0.9874561 ,\n",
       "       -0.57459784], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1e0476c5280>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1e0476c5c40>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1e047f5f890>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1e0479a2380>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1e0479d6740>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1e047f5f820>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Adding another component in pipeline\n",
    "\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a complex component with matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "with open(\"exercises/en/iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# define the pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# definer the matcher with the vocab of the pipeline\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and create docs with matched entities\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "\n",
    "docs = []\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    print(spans)\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)\n",
    "\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy init config ./config.cfg --lang en --pipeline ner\n",
    "# !cat ./config.cfg\n",
    "# !python -m spacy train ./exercises/en/config_gadget.cfg --output ./output --paths.train ./exercises/en/train_gadget.spacy --paths.dev ./exercises/en/dev_gadget.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
