{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best pratices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove punctuation: Punctuation like commas, periods, and quotation marks are not always useful for NLP tasks and can add noise to the data.\n",
    "\n",
    "* Remove HTML tags: If working with web data, it's important to remove any HTML tags before analyzing the text.\n",
    "\n",
    "* Remove special characters: Special characters like @, #, or $ can also add noise to the data and should be removed.\n",
    "\n",
    "* Remove stop words: Stop words are common words like \"the,\" \"and,\" and \"a\" that don't add much meaning to the text and can be removed to reduce noise.\n",
    "\n",
    "* Remove numbers: If numbers are not important to the analysis, they should be removed.\n",
    "\n",
    "* Correct spelling mistakes: Misspelled words can be corrected using techniques like spell-checking or pattern matching.\n",
    "\n",
    "* Handle capitalization: Depending on the task, capitalization may or may not be important. It's important to consider this and decide whether to convert all text to lowercase or preserve the original capitalization.\n",
    "\n",
    "* Handle abbreviations: Abbreviations should be expanded to their full form to ensure their meaning is captured.\n",
    "\n",
    "* Handle slang and informal language: Slang and informal language can add complexity to the analysis and should be handled appropriately, depending on the task.\n",
    "\n",
    "* Tokenize and lemmatize: Text should be tokenized into smaller chunks like words or phrases, and words should be lemmatized to their base form to ensure consistency and reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'includes', 'stop', 'words', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lfroes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"This is an example sentence that includes some stop words.\"\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Vectorization in NLP refers to the process of converting text data into numerical vectors that can be used as input for machine learning models. This process involves encoding each word or phrase in a text corpus as a vector, where each dimension of the vector represents a particular feature or attribute of the word or phrase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bag of Words (BoW): This approach is useful when we have a large corpus of text and the focus is on keyword-based analysis. BoW can be used for tasks such as document classification, spam detection, and sentiment analysis.\n",
    "\n",
    "* TF-IDF: This approach is similar to BoW, but it gives more importance to rare words that are discriminative for a specific document. TF-IDF is useful when we want to focus on important words that are not very common in the corpus. It can be used for tasks such as document classification, search engines, and recommender systems.\n",
    "\n",
    "* Word embeddings: This approach is useful when we want to capture the semantic relationships between words. Word embeddings are low-dimensional dense vectors that represent the meaning of words. They can be used for tasks such as language modeling, named entity recognition, and sentiment analysis.\n",
    "\n",
    "* BERT: This approach is useful when we want to capture the context-dependent meaning of words. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that can be fine-tuned for various NLP tasks such as sentiment analysis, question answering, and text classification.\n",
    "\n",
    "* GloVe: This approach is similar to word embeddings but uses a different technique for training the vectors. GloVe (Global Vectors for Word Representation) is trained on the co-occurrence statistics of words in a corpus. It can be used for similar tasks as word embeddings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of word\n",
    "\n",
    "BoW is a good choice for tasks where word frequency matters and the context is not as important. For example, spam detection, sentiment analysis, or topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# define the dataset\n",
    "X = ['this is a good movie', 'this is a bad movie', 'I like this movie', 'I hate this movie']\n",
    "y = [1, 0, 1, 0]\n",
    "\n",
    "# create the BoW representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "# train a classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_bow, y)\n",
    "\n",
    "# predict the sentiment of a new text\n",
    "new_text = ['this is a great movie']\n",
    "new_text_bow = vectorizer.transform(new_text)\n",
    "predicted_sentiment = clf.predict(new_text_bow)[0]\n",
    "\n",
    "print(predicted_sentiment)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "TF-IDF is a good choice when we want to weight the importance of a word in a document based on its frequency in the corpus. It is useful in applications such as document classification, information retrieval, or keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# define the dataset\n",
    "X = ['this is a good movie', 'this is a bad movie', 'I like this movie', 'I hate this movie']\n",
    "y = ['positive', 'negative', 'positive', 'negative']\n",
    "\n",
    "# create the TF-IDF representation\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# train a classifier\n",
    "clf = SVC()\n",
    "clf.fit(X_tfidf, y)\n",
    "\n",
    "# predict the class of a new text\n",
    "new_text = ['this is a great movie']\n",
    "new_text_tfidf = vectorizer.transform(new_text)\n",
    "predicted_class = clf.predict(new_text_tfidf)[0]\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "\n",
    "Word embeddings are useful when we want to capture the meaning of words and the context in which they are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# define the dataset\n",
    "X = ['this is a good movie', 'this is a bad movie', 'I like this movie', 'I hate this movie']\n",
    "y = [1, 0, 1, 0]\n",
    "\n",
    "# load the pre-trained embedding model\n",
    "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "\n",
    "# convert the text to embeddings\n",
    "X_embeddings = embed(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 50), dtype=float32, numpy=\n",
       "array([[ 0.20583047,  0.18753016,  0.1268343 ,  0.09016597, -0.1543585 ,\n",
       "        -0.10795088,  0.16558027,  0.06119017, -0.20971456,  0.07139651,\n",
       "        -0.03833621, -0.08680907,  0.04552191, -0.02194868,  0.13046873,\n",
       "        -0.12728618, -0.1495209 ,  0.18820915,  0.03948668, -0.29305086,\n",
       "        -0.02014771, -0.22412075,  0.12644166,  0.0870514 , -0.08078168,\n",
       "        -0.00701902, -0.415201  , -0.06377582,  0.21221496,  0.04958345,\n",
       "        -0.17487982,  0.12846486, -0.03652546, -0.12215738, -0.0015185 ,\n",
       "         0.05498244,  0.25573802, -0.09744763, -0.04855936, -0.2361716 ,\n",
       "         0.08039324, -0.02337173, -0.06055538,  0.03737477, -0.28236404,\n",
       "        -0.27627096, -0.04884144, -0.04169737,  0.05408333,  0.01726649],\n",
       "       [ 0.22665966,  0.12342206,  0.00879236,  0.06335424, -0.12636712,\n",
       "        -0.19964664,  0.18133612,  0.03515789, -0.13501574,  0.09100834,\n",
       "         0.02877173, -0.00904876,  0.03736711, -0.08183728,  0.01896096,\n",
       "        -0.16278717, -0.20696107,  0.14243598,  0.02036276, -0.28126782,\n",
       "         0.01335658, -0.13451222,  0.22987807,  0.114016  , -0.08412866,\n",
       "        -0.0179373 , -0.40019724, -0.0634312 ,  0.25229695,  0.02300838,\n",
       "        -0.22072741,  0.04974015, -0.03426081, -0.12143179,  0.01243285,\n",
       "         0.09758102,  0.20947827, -0.09088232, -0.03397707, -0.15891847,\n",
       "         0.06815644, -0.03877518, -0.03234315,  0.03991077, -0.34939274,\n",
       "        -0.28522035, -0.05510631, -0.07804935,  0.14075133, -0.06605069],\n",
       "       [ 0.07778277,  0.04684148, -0.28094456,  0.4256207 , -0.05173326,\n",
       "        -0.0258597 ,  0.05179566, -0.19683954, -0.17535669,  0.11801724,\n",
       "        -0.02635611, -0.02301309, -0.09530608,  0.07232662,  0.00348042,\n",
       "         0.04845591,  0.05654622,  0.1486232 , -0.02092993, -0.1970374 ,\n",
       "        -0.01078024, -0.02564417,  0.12558624,  0.00517325, -0.14626658,\n",
       "         0.06754916, -0.41334546, -0.01906511,  0.13496989,  0.02433002,\n",
       "        -0.23029703, -0.05781772, -0.06942347,  0.19365051,  0.07203194,\n",
       "         0.27212113,  0.23841012,  0.06437163, -0.18142599, -0.18083252,\n",
       "         0.01624359,  0.02868058, -0.1505725 , -0.02267272, -0.24000612,\n",
       "         0.05040375, -0.20410925, -0.10473073,  0.15262376,  0.1127179 ],\n",
       "       [ 0.12735304,  0.03132109, -0.20181948,  0.25471082,  0.02453477,\n",
       "        -0.01167366, -0.03595093, -0.28527457, -0.10126282,  0.08331248,\n",
       "         0.06318519, -0.00178006, -0.10611597,  0.05175281, -0.06132956,\n",
       "         0.03593881, -0.13904248,  0.13285126, -0.14339691, -0.19387406,\n",
       "         0.00725409, -0.08883395,  0.30449054, -0.12653664, -0.13484816,\n",
       "         0.07923483, -0.41337818, -0.04800935,  0.12763669, -0.02196075,\n",
       "        -0.13100928, -0.0352507 , -0.10137847,  0.20245567,  0.18247983,\n",
       "         0.1616772 ,  0.19378516,  0.01456436, -0.20443664, -0.12747629,\n",
       "         0.13317326,  0.0009077 , -0.14448085, -0.0578664 , -0.24444437,\n",
       "        -0.04250093, -0.30013928, -0.07292242,  0.32482374, -0.08954549]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP models, such as neural networks, can benefit from the use of batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minmaxscaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
